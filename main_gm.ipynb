{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "# !pip install --upgrade --user ase\n",
    "# !pip install ipywidgets \n",
    "# !pip install ipympl\n",
    "# !conda install nglview -c conda-forge -y\n",
    "# !pip install --upgrade --user asap3\n",
    "# !jupyter-nbextension enable nglview --py --sys-prefix\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate normal distribution\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_normal(x, mean, variance):\n",
    "    \"\"\"pdf of the univariate normal distribution.\"\"\"\n",
    "    return ((1. / np.sqrt(2 * np.pi * variance)) * \n",
    "            np.exp(-(x - mean)**2 / (2 * variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c40ff5e44384c7d9b039e9008b3933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7abfda33264d5ab1b8baa554830a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu', max=3.0, min=-3.0), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up plot\n",
    "univariate_plot(univariate_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d \\lvert\\Sigma\\rvert}} \\exp{ \\left( -\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{\\mu}, \\Sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal(x, d, mean, covariance):\n",
    "    \"\"\"pdf of the multivariate normal distribution.\"\"\"\n",
    "    x_m = x - mean\n",
    "    return (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) * \n",
    "            np.exp(-(np.linalg.solve(covariance, x_m).T.dot(x_m)) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Multivariate normal distribution\n",
    "$$\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix}\n",
    "1 & 0.8 \\\\\n",
    "0.8 & 1\n",
    "\\end{bmatrix}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a530ce3405684af3b4315c6da32f4b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71f0969538841fa974f51b0049d3582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='C', max=0.99, step=0.01), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "\n",
    "multivariate_plot(multivariate_normal, nb_of_x=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal and Conditional normal distributions\n",
    "\n",
    "If both $\\mathbf{x}$ and $\\mathbf{y}$ are [jointly normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Joint_normality) random vectors defined as:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{x} \\\\\n",
    "\\mathbf{y} \n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_{\\mathbf{x}} \\\\\n",
    "\\mu_{\\mathbf{y}}\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "A & C \\\\\n",
    "C^T & B\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "= \\mathcal{N}(\\mu, \\Sigma)\n",
    ", \\qquad \n",
    "\\Sigma^{-1} = \\Lambda = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{A} & \\tilde{C} \\\\\n",
    "\\tilde{C}^T & \\tilde{B}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The [conditional distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution) of $\\mathbf{x}$ given $\\mathbf{y}$ is defined as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{y}) = \\mathcal{N}(\\mu_{x|y}, \\Sigma_{x|y})\n",
    "$$\n",
    "\n",
    "With:\n",
    "$$\\begin{split}\n",
    "\\Sigma_{x|y} & = A - CB^{-1}C^\\top = \\tilde{A}^{-1} \\\\\n",
    "\\mu_{x|y} & = \\mu_x + CB^{-1}(\\mathbf{y}-\\mu_y)\n",
    "\\end{split}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\ \n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "By using thr following idendty:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "M^{-1}=\n",
    "\\begin{pmatrix}\n",
    "A & B \\\\ \n",
    "C & D\n",
    "\\end{pmatrix}^{-1}=\n",
    "\\begin{pmatrix}\n",
    "I & 0 \\\\ \n",
    "-C^{-1}C & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "(M/D)^{-1} & 0 \\\\ \n",
    "0 & D^{-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "I & -BD^{-1} \\\\ \n",
    "0 & I\n",
    "\\end{pmatrix}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "We obtainig the folowing:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\ \n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "I & 0 \\\\ \n",
    "-\\Sigma_{22}^{-1} \\Sigma_{21} & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "(\\Sigma/\\Sigma_{22})^{-1} & 0 \\\\ \n",
    "0 & \\Sigma_{22}^{-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "I & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\ \n",
    "0 & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "And finally:\n",
    "\n",
    "$\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)) + (x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "$\n",
    "\n",
    "And plugin back into the first Eq:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)) + (x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "and from the axiom of probability we obtain the follwoing equlaity:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = p(x_1|x_2)p(x_2)= exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right] = \n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "p(x_2)\n",
    "\\end{split} \\\\ \n",
    "\\Rightarrow p(x_1|x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\\\\n",
    "\\mu_{1|2} = \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)\n",
    "\\Sigma_{1|2} = (\\Sigma/\\Sigma_{22})=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde4f315351f45aeb34069ca8866413d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f7c814003d493daa6493acdca6b557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='x', max=1.0, min=-1.0), Flo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "condition_plot(nb_of_x=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process\n",
    "Gaussian process (GP) Is a method predicting $y^*$ for a given $x^*$ and getting: $y_i=f(x_i )$. \n",
    "GP assumes that p(f(x_1),...,f(x_N  )) is jointly Gaussian, i.e., the value of a new point m is defined as a multidimensional gaussian with $\\mu(x)$ and $\\Sigma(x)$, where $\\Sigma(x)$ is calculated as $\\Sigma_{ij}=k(x_i,x_j)$, and $k$ is the kernel function.<br/>\n",
    "Which can be regarded as a \"distance\" function which defines how strongly the value f(x_i) is coupled to point f(x_j). \n",
    "When trying to predict a new point x^* we use prior data points, calculating the new kernel values, and finally obtaining the following new multidimensional Gaussian distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "f \\\\ \n",
    "f^*\n",
    "\\end{pmatrix}~\n",
    "\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\mu \\\\ \n",
    "\\mu^*\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "k & k^*\\\\ \n",
    "{k^{*}}^{T} & k^{**}\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br/>\n",
    "$f$ - Vector of all observed $y_i$ values, $f=y_i=f(x_i)$.<br/>\n",
    "$f^*$ - Prediction function for point $x^*$, $f^*=y^*=f(x^*)$.<br/>\n",
    "$\\mu$ - Vector of all observed mean values.<br/>\n",
    "$\\mu^*$- Mean values for the prediction for x^*. <br/>\n",
    "$k$ - Covariance matrix of all observed points<br/>\n",
    "$k^*$ - Kernel vector $k^*=k(x^*,x_i )$<br/>\n",
    "$k^{**}$ - Self kernel vector $k^{**}=k(x^*,x^* )$<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Conditional \n",
    "$$f(x^*)=\\mu^* +{{k}^{*}}^{T}  k^{-1}(y-\\mu)$$ <br/>\n",
    "For simplicity, we can define the mean values to be zero<br/>\n",
    "$$f(x^*) ={k^*}^{T}k^{-1}y=\\sum_{i=1}^{N}\\alpha_ik(x_i, x^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Most Popular Kernal\n",
    "Here, we will use the squared exponential kernel, also known as Gaussian kernel or RBF kernel\n",
    "$$\n",
    "\\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2}\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)\\right)\\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://juanitorduz.github.io/gaussian_process_reg/\n",
    "# http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes\n",
    "    a covariance matrix from points in X1 and X2.\n",
    "\n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    sqdist = np.sum(X1 ** 2, 1).reshape(-1, 1) + np.sum(X2 ** 2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f ** 2 * np.exp(-0.5 / l ** 2 * sqdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Conditional on A Nosey Dataset\n",
    "If we have a training dataset with noisy function values $\\mathbf{y} = \\mathbf{f} + \\boldsymbol\\epsilon$ where noise $\\boldsymbol\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_y^2 \\mathbf{I})$ is independently added to each observation then the predictive distribution is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) &= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*) \\\\\n",
    "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y} \\\\\n",
    "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\mathbf{K}_y = \\mathbf{K} + \\sigma_y^2\\mathbf{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "    '''\n",
    "    Computes the suffifient statistics of the GP posterior predictive distribution\n",
    "    from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "    Args:\n",
    "        X_s: New input locations (n x d).\n",
    "        X_train: Training locations (m x d).\n",
    "        Y_train: Training targets (m x 1).\n",
    "        l: Kernel length parameter.\n",
    "        sigma_f: Kernel vertical variation parameter.\n",
    "        sigma_y: Noise parameter.\n",
    "\n",
    "    Returns:\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "    '''\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y ** 2 * np.eye(len(X_train))\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\n",
    "    K_inv = inv(K)\n",
    "\n",
    "\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "\n",
    "\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "\n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b949772aa4a6fa28e5d2fb0712720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()\n",
    "noise = 0.1\n",
    "x = [-3, -2 , -1,  1, 2 , 3, 4]\n",
    "def f(x):\n",
    "    func = np.cos(x)\n",
    "    return func\n",
    "    \n",
    "gaussian_process(x, f, noise, posterior_predictive, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a278436b25740e4b6ff2dade81d4c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "noise = 0.1\n",
    "kernal = ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "def f(x):\n",
    "    func = np.sin(x)\n",
    "    return func\n",
    "\n",
    "gaussian_process_interactive(f, noise, kernal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Force Field \n",
    "a straightforward formulation of a vector-valued estimator\n",
    "takes the form:\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x), \\dots, \\hat{f}_N\\end{bmatrix}^{T}\n",
    "$$\n",
    "$\\mathbf{\\hat{f}}: \\mathbb{R}^N\\rightarrow\\mathbb{R}^N$ where each component: ${\\hat{f}_i}: \\mathbb{R}^N\\rightarrow\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lattice(symbols='Cu32', pbc=True, cell=[7.22, 7.22, 7.22], momenta=..., calculator=EMT(...))\n",
      "Energy per atom: Epot = -0.006eV  Ekin = 0.081eV (T=626K)  Etot = 0.075eV, time elapsed:  0\n",
      "Energy per atom: Epot = -0.006eV  Ekin = 0.081eV (T=626K)  Etot = 0.075eV, time elapsed:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.close('all')\n",
    "\"\"\"Demonstrates molecular dynamics with constant temperature.\"\"\"\n",
    "import os\n",
    "from ase.lattice.cubic import FaceCenteredCubic\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.io.trajectory import Trajectory\n",
    "from ase import io\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase import units\n",
    "import time\n",
    "use_asap = False\n",
    "from ase.calculators.emt import EMT\n",
    "size = 2\n",
    "\n",
    "T = 300  # Kelvin\n",
    "atom_type = 'Cu'\n",
    "\n",
    "# Set up a crystal\n",
    "atoms = FaceCenteredCubic(directions=[[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n",
    "                          symbol=atom_type,\n",
    "                          size=(size, size, size),\n",
    "                          pbc=True)\n",
    "\n",
    "# Describe the interatomic interactions with the Effective Medium Theory\n",
    "atoms.calc = EMT()\n",
    "# Set the momenta corresponding to T=300K\n",
    "MaxwellBoltzmannDistribution(atoms, 600 * units.kB)\n",
    "\n",
    "# We want to run MD with constant energy using the Langevin algorithm\n",
    "# with a time step of 5 fs, the temperature T and the friction\n",
    "# coefficient to 0.02 atomic units.\n",
    "dyn = Langevin(atoms, 1 * units.fs, T * units.kB, 0.002)\n",
    "\n",
    "t0 = time.time()\n",
    "def printenergy(a=atoms):  # store a reference to atoms in the definition.\n",
    "    \"\"\"Function to print the potential, kinetic and total energy.\"\"\"\n",
    "    epot = a.get_potential_energy() / len(a)\n",
    "    ekin = a.get_kinetic_energy() / len(a)\n",
    "    print('Energy per atom: Epot = %.3feV  Ekin = %.3feV (T=%3.0fK)  '\n",
    "          'Etot = %.3feV, time elapsed:%3.0f' % (epot, ekin, ekin / (1.5 * units.kB), epot + ekin, time.time()-t0))\n",
    "    \n",
    "dyn.attach(printenergy, interval=100)\n",
    "\n",
    "# We also want to save the positions of all atoms after every 100th time step.\n",
    "traj = Trajectory('moldyn.traj', 'w', atoms)\n",
    "dyn.attach(traj.write, interval=1)\n",
    "print(atoms)\n",
    "\n",
    "# Now run the dynamics\n",
    "try:\n",
    "    os.makedirs('snapshots')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "printenergy()\n",
    "steps = 100 - 1\n",
    "dyn.run(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdada31837a545559dde4faaa3a3ae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ff2a030354476bb207ae603fd1be8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(NGLWidget(max_frame=99), VBox(children=(Dropdown(description='Show', options=('All', 'Cu'), val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ase.io.trajectory import Trajectory\n",
    "from ase.visualize import ngl\n",
    "from ase import Atom, Atoms\n",
    "traj = Trajectory('moldyn.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of mapping to scalar outputs, we can alternatively model the covariance\n",
    "function as a matrix $k : \\chi × \\chi → \\mathbb{R}^N\\rightarrow\\mathbb{R}^{N\\times N}$ that expresses the interaction among\n",
    "multiple output components. Together with a vector-valued mean function $\\mu : \\chi →\n",
    "\\mathbb{R}^N$ , we can then sample realizations of vector-valued functions from the GP\n",
    "$$\n",
    "\\mathbf{\\hat{f}} ∼ \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)}, \\mathbf{k(x,x^{'})}\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the navie approch:\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x), \\dots, \\hat{f}_N\\end{bmatrix}^{T}\n",
    "$$\n",
    "$$\n",
    "\\hat{f}_i(x) = \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_i}, \\mathbf{k(x,x^{'})_i}\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x) \\\\ \\dots \\\\ \\hat{f}_N(x)\\end{bmatrix}\n",
    "=\\begin{bmatrix}\\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_1}, \\mathbf{k(x,x^{'})_1}\\end{bmatrix} \\\\ \n",
    "\\dots\n",
    "\\\\ \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_N}, \\mathbf{k(x,x^{'})_N}\\end{bmatrix}\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = Trajectory('moldyn.traj', 'r', atoms)\n",
    "eps = 0.1\n",
    "start, every = 1, 1\n",
    "y, x = [], []\n",
    "y_test, x_test = [], []\n",
    "for i, config in enumerate(traj):\n",
    "    if i % every == 0 and i > start:\n",
    "        if i <= (len(traj) + start)/2:\n",
    "            y.append(config.get_forces().reshape([1, -1]))\n",
    "            x.append(config.get_positions().reshape([1, -1]))\n",
    "        else:\n",
    "            x_test.append(config.get_positions().reshape([1, -1]))\n",
    "            y_test.append(config.get_forces().reshape([1, -1]))\n",
    "    \n",
    "x, y = np.concatenate(x, axis=0), np.concatenate(y, axis=0)\n",
    "x_test, y_test = np.concatenate(x_test, axis=0), np.concatenate(y_test, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr(gpr, x, y, x_test, y_test):\n",
    "    y_hat_train = gpr.predict(x)\n",
    "    y_hat_train_norm = np.linalg.norm(y_hat_train, axis=1)\n",
    "    y_train_norm = np.linalg.norm(y, axis=1)\n",
    "    y_hat = gpr.predict(x_test)\n",
    "    y_hat_norm = np.linalg.norm(y_hat, axis=1)\n",
    "    y_test_norm = np.linalg.norm(y_test, axis=1)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, axis = plt.subplots(ncols=1, nrows=2, sharex=True)\n",
    "    x1 = np.arange(len(y_hat_train_norm))\n",
    "    axis[0].plot(x1, y_hat_train_norm, label='predicted on train')\n",
    "    axis[0].plot(x1, y_train_norm, label='training')\n",
    "    x2 = np.arange(len(y_hat_norm))+len(y_hat_train_norm)\n",
    "    axis[0].plot(x2, y_hat_norm, label='predicted')\n",
    "    axis[0].plot(x2, y_test_norm, label='ground truth')\n",
    "    axis[0].set_xticks([])\n",
    "    axis[0].set_title('The force norm')\n",
    "    axis[0].legend()\n",
    "\n",
    "    x1 = np.arange(len(y_hat_train_norm))\n",
    "    axis[1].plot(x1, y_hat_train_norm-y_train_norm, label='diffrence on train')\n",
    "    x2 = np.arange(len(y_hat_norm))+len(y_hat_train_norm)\n",
    "    axis[1].plot(x2, y_hat_norm-y_test_norm, label='diffrence on test')\n",
    "    axis[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685167f497c845cf8d63ccb54b12f24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "noise = 0.2\n",
    "kernal= ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some tricks\n",
    "##### change the kernal:\n",
    "##### The Matérn kernel\n",
    "For our application, we considered a subclass from the parametric\n",
    "Matérn family (22–24) of (isotropic) kernel functions\n",
    "$$\n",
    "k: C_{\\mu=n+\\frac{1}{2}}(d)=exp{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}P_{n}(d)\n",
    "$$\n",
    "$$\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dd92ab564b4f8daa3732d0cbc58b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
    "noise = 0.2\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalize y\n",
    "$$\n",
    "\\hat{y}_{train} = \\frac{y_{train}-E[y_{train}]}{\\sqrt{Var[y_{train}]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16b621abe174f36ade9c2f2ca3c16af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = 0.2\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2, normalize_y=True)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roto-translational invariance\n",
    "Covariance functions remain valid under any transformation of their domain\n",
    ") is again a kernel function. A rather trivial implication is that all\n",
    "invariances of that input transformation are inherited, providing yet another opportunity\n",
    "to characterize the properties of the predictor [92].\n",
    "\n",
    "The so-called Coulomb matrix representation [7] goes one step further and represents\n",
    "each pair of nuclei in terms their Coulomb interaction instead of a simple distance. The\n",
    "Coulomb energy is the only nuclei-nuclei interaction term in the Hamiltonian and empirically a good starting point for inference about molecular properties [9]. We use a slight\n",
    "variation of this descriptor for our purpose, whereby atoms of different type interact on a\n",
    "normalized scale,\n",
    "$$\n",
    "D_{ij}=\\begin{matrix}\\lVert{r_i - r_j}\\lVert & i > j \\\\ 0 & i \\leq j \\end{matrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7138cb79eb42bcabd8086131c1a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "def x_to_d(x):\n",
    "    d = []\n",
    "    for xi in x:\n",
    "        xi = xi.reshape([-1, 3])\n",
    "        d.append(pdist(xi))\n",
    "    return np.stack(d, axis=0)\n",
    "\n",
    "d, d_test = x_to_d(x), x_to_d(x_test)\n",
    "noise = 0.5\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2, normalize_y=True)\n",
    "gpr.fit(d, y)\n",
    "plot_gpr(gpr, d, y, d_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incoding physical insight to the model\n",
    "In this setting, the corresponding RKHS is vector-valued and it has been shown\n",
    "that the representer theorem continues to hold. Each component of the kernel\n",
    "function $k_{ij}$ specifies a covariance between a pair of outputs $f_i(x)$ and $f_j(x)$, which\n",
    "makes it straightforward to impose linear constraints $g(x) = \\hat{G}[\\mathbf{f(x)}]$ on the GP\n",
    "prior\n",
    "$$\n",
    "\\mathbf{\\hat{g}(x)} ∼ \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\hat{G}\\mu(x)}, \\hat{G}\\mathbf{k(x,x^{'})}{\\hat{G}'}^{T}\\end{bmatrix}\n",
    "\\\\\n",
    "for A, B\\; linear\\; operators\\\\\n",
    "Cov[Ax,By]=ACov[x,y]B^{T}\\\\\n",
    "E[Ax] = AE[x]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we aim to construct a GP that inherits the correct structure of a conservative\n",
    "force field to ensure integrability, so that the corresponding energy potential can\n",
    "be recovered from the same model. We start by considering, that the force field\n",
    "estimator $\\mathbf{\\hat{f}_{F}(x)}$ and the PES estimator $\\hat{f}_{E}(x)$ are related via some operator $\\hat{G}$ . To\n",
    "impose energy conservation, we require that the curl vanishes  for every\n",
    "input to the transformed energy model:\n",
    "$$\n",
    "\\nabla\\times \\hat{G}\\begin{bmatrix}\\hat{f}_E\\end{bmatrix} = \\mathbf{0}\n",
    "$$\n",
    "As expected, this is satisfied by the derivative operator Gˆ = ∇ or, in the case of\n",
    "energies and forces, the negative gradient operator\n",
    "$$\n",
    "\\mathbf{\\hat{f}_{F}(x)}=\\hat{G}\\begin{bmatrix}\\hat{f}_E\\end{bmatrix}=-\\nabla\\hat{f}_E\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since differentiation is a linear operator, the result is another GP with realizations $\\mathbf{f_F}: \\chi^{3N}\\rightarrow \\mathbb{R}^{3N}$\n",
    "$$\n",
    "\\mathbf{\\hat{f}_F(x)} ∼ \\mathcal{GP}\\begin{bmatrix}\\mathbf{-\\nabla\\mu(x)}, \\nabla_x\\mathbf{k(x,x^{'})}{ \\nabla_{x'}}^{T}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{k}{ \\nabla}^{T}=Hess_x(k)\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}Hess_s(k)\\end{bmatrix}_{ij}=\\frac{\\partial^2 k}{\\partial x_i \\partial x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Hessian kernel gives rise to the following gradient domain machine\n",
    "learning [88, 89] force model as the posterior mean of the corresponding GP:<br>\n",
    "he trained force field estimator collects the contributions of the\n",
    "partial derivatives 3N of all training points M to compile the prediction.\n",
    "It takes the form\n",
    "$$\n",
    "\\mathbf{\\hat{f}_F(x)}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j}\\nabla k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$\n",
    "    \n",
    "Because the trained model is a (fixed) linear combination of kernel functions,\n",
    "integration only affects the kernel function itself. The corresponding expression for\n",
    "the energy predictor\n",
    "$$\n",
    "\\mathbf{\\hat{f}_E(x)}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j} k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;37;40m[INFO]\u001b[0m Overwriting existing dataset file.\n",
      "Writing dataset to 'moldyn.npz'...\n",
      "Number geometries found: 100\n",
      "\n",
      "\u001b[1;32;40m[DONE]\u001b[0m\n",
      "\u001b[1;37mDataset properties\u001b[0m\n",
      "  Name:              moldyn (32 atoms)\n",
      "  Theory:            unknown\n",
      "  Size:              100 data points\n",
      "  Lattice:           a    b    c   \n",
      "                     7.22 0    0   \n",
      "                     0    7.22 0   \n",
      "                     0    0    7.22\n",
      "    Lengths:         a = 7.22, b = 7.22, c = 7.22\n",
      "    Angles [deg]:    alpha = 90, beta = 90, gamma = 90\n",
      "  Energies [eV]:\n",
      "    Range:           -0.182 |--   1.98   --| 1.8      \n",
      "    Mean:            1.172\n",
      "    Variance:        0.331\n",
      "  Forces [eV/Ang]:\n",
      "    Range:           -1.42 |--   2.81   --| 1.38     \n",
      "    Mean:            0.000\n",
      "    Variance:        0.192\n",
      "  Fingerprint:       b'862828807d3ef182d5c124408a79fd28'\n",
      "\n",
      "\u001b[1;37mExample geometry\u001b[0m (no. 60, chosen randomly)\n",
      "  Copy&paste the string below into Jmol (www.jmol.org), Avogadro (www.avogadro.cc), etc. to\n",
      "  visualize a geometry from this dataset. A new example will be drawn on each call.\n",
      "\n",
      "\u001b[90m  ---- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE ----\u001b[0m\n",
      "  32\n",
      "  Lattice=\"7.22 0 0 0 7.22 0 0 0 7.22\" Energy=1.50738856223 Properties=species:S:1:pos:R:3:forces:R:3\n",
      "  Cu  0.0623777  -0.185777   0.109279  -0.383051   0.449426    -0.761796 \n",
      "  Cu  1.90089     1.96264   -0.0172259 -0.459883  -0.346199    -0.189438 \n",
      "  Cu  1.71631     0.187313   1.78079    0.372623  -0.727201     0.348167 \n",
      "  Cu  0.00269736  1.90532    1.60759    0.0487265 -0.281053     0.459591 \n",
      "  Cu  3.46981    -0.020823   0.0206689  0.586481   0.173106     0.13402  \n",
      "  Cu  5.26804     1.68835   -0.162851   0.470337   0.118737     0.720223 \n",
      "  Cu  5.60569     0.0308094  1.83911   -0.630167  -0.0470819    0.0361492\n",
      "  Cu  3.62308     1.69203    1.8591     0.0600001  0.643401    -0.280296 \n",
      "  Cu -0.107248    3.77551    0.0607507  0.466271  -0.280293    -0.453349 \n",
      "  Cu  1.97764     5.37046   -0.0790238 -0.848946  -0.256412     0.777264 \n",
      "  Cu  1.89738     3.61822    1.86591   -0.32744    0.000537904 -0.407014 \n",
      "  Cu  0.0667609   5.36246    1.926     -0.190327   0.249982    -0.573629 \n",
      "  Cu  3.68352     3.57449    0.0180071 -0.125692   0.185379    -0.285121 \n",
      "  Cu  5.30296     5.50888    0.0623848  0.14076   -0.440625    -0.0145797\n",
      "  Cu  5.39357     3.44342    1.76812   -0.339425   0.933599     0.251392 \n",
      "  Cu  3.60231     5.3535     1.88917    0.517005   0.109518    -0.542906 \n",
      "  Cu  0.0769529   0.0864057  3.70683   -0.781866  -0.0533141   -0.498875 \n",
      "  Cu  1.72        1.92852    3.62118    0.260961  -0.808383     0.0127793\n",
      "  Cu  1.62948     0.062817   5.51561    0.797951  -0.255179    -0.25996  \n",
      "  Cu -0.0404404   1.87389    5.5049    -0.0627234  0.0712435    0.107938 \n",
      "  Cu  3.71901    -0.0794131  3.47716   -0.219317   0.281116     0.31052  \n",
      "  Cu  5.41337     1.84285    3.59305    0.409439  -0.367644     0.44438  \n",
      "  Cu  5.39443    -0.114026   5.42474    0.612577   0.225704    -0.232636 \n",
      "  Cu  3.55653     1.52938    5.25856   -0.041771   1.01543      0.387577 \n",
      "  Cu  0.00254174  3.61277    3.59309   -0.124071  -0.202644     0.136789 \n",
      "  Cu  1.83874     5.58236    3.52737   -0.227097  -0.545957     0.731327 \n",
      "  Cu  1.78512     3.67713    5.39721    0.264093  -0.446446     0.0244188\n",
      "  Cu  0.0616767   5.34583    5.41163   -0.448679   0.470356    -0.0690065\n",
      "  Cu  3.57827     3.5685     3.71162    0.263595   0.063908    -0.512125 \n",
      "  Cu  5.50439     5.36765    3.63448   -0.310682   0.0571952   -0.35373  \n",
      "  Cu  5.34727     3.63313    5.423      0.275761  -0.075976     0.11572  \n",
      "  Cu  3.58686     5.45541    5.29178   -0.0254443  0.0857687    0.436205 \n",
      "\u001b[90m  ---- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE --- CUT HERE ----\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sgdml.cli import _print_dataset_properties, _print_model_properties, _print_task_properties\n",
    "from data_utils import from_traj\n",
    "dataset_path = 'moldyn'\n",
    "dataset = from_traj(f'{dataset_path}.traj', overwrite=True)\n",
    "dataset_path = f'{dataset_path}.npz'\n",
    "# test = np.load('ethanol_dft.npz')\n",
    "_print_dataset_properties(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call, check_output\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "def run_command(command):\n",
    "    process = subprocess.Popen(shlex.split(command), stdout=subprocess.PIPE)\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip().decode('ascii'))\n",
    "    rc = process.poll()\n",
    "    return rc\n",
    "\n",
    "def create(dataset_path, n_train, n_valid,\n",
    "           task_dir = None,\n",
    "          valid_dataset_path=None, \n",
    "          sigs=None,\n",
    "          overwrite=True,\n",
    "          use_torch=False,\n",
    "          max_processes=None):\n",
    "    execute_line = f'sgdml create {dataset_path} {n_train} {n_valid} '\n",
    "    if task_dir is not None:\n",
    "        execute_line += f'--task_dir {task_dir}'\n",
    "    if sigs is not None:\n",
    "        sig = '--sig ' + ' '.join([str(s) for s in sigs])\n",
    "        execute_line += f'{sig} '\n",
    "    if overwrite:\n",
    "        execute_line += '-o ' \n",
    "    if use_torch:\n",
    "        execute_line += '--torch '\n",
    "    if max_processes is not None:\n",
    "        execute_line += f'-p {max_processes}'\n",
    "    return execute_line\n",
    "#     call(execute_line, shell=True)\n",
    "#     print(check_output(execute_line, shell=True).decode('ascii'))\n",
    "\n",
    "def train(task_dir, valid_dataset_path,\n",
    "          overwrite=True,\n",
    "          use_torch=False,\n",
    "          max_processes=None):\n",
    "    execute_line = f'sgdml train {task_dir} {valid_dataset_path} '\n",
    "    if overwrite:\n",
    "        execute_line += '-o ' \n",
    "    if use_torch:\n",
    "        execute_line += '--torch '\n",
    "    if max_processes is not None:\n",
    "        execute_line += f'-p {max_processes}'\n",
    "    return execute_line\n",
    "#     call(execute_line, shell=True)\n",
    "#     print(check_output(execute_line, shell=True).decode('ascii'))\n",
    "    \n",
    "def all(dataset_path, n_train, n_valid, n_test=None, \n",
    "          valid_dataset_path=None, \n",
    "          train_dataset_path=None,\n",
    "          sigs=None,\n",
    "          overwrite=True,\n",
    "          use_torch=False,\n",
    "          max_processes=None):\n",
    "    execute_line = f'sgdml all {dataset_path} {n_train} {n_valid} '\n",
    "    if n_train is not None:\n",
    "        execute_line += f'{n_train} '\n",
    "    if sigs is not None:\n",
    "        sig = '--sig ' + ' '.join([str(s) for s in sigs])\n",
    "        execute_line += f'{sig} '\n",
    "    if overwrite:\n",
    "        execute_line += '-o ' \n",
    "    if use_torch:\n",
    "        execute_line += '--torch '\n",
    "    if max_processes is not None:\n",
    "        execute_line += f'-p {max_processes}'\n",
    "    script_name = 'run_me.sh'\n",
    "    with open(script_name, 'w') as f:\n",
    "        f.write('#!/usr/bin/sh\\n')\n",
    "        f.write(execute_line)\n",
    "    return script_name\n",
    "#     call(execute_line, shell=True)\n",
    "#     run_command(execute_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please run \"run_me.sh\" from the terminal\n"
     ]
    }
   ],
   "source": [
    "# train(dataset_path, 10, 10)\n",
    "from sgdml import cli\n",
    "from sgdml import utils\n",
    "n_train = 10\n",
    "n_valid = 10\n",
    "file = all(dataset_path, n_train, n_valid)\n",
    "print(f'please run \\\"{file}\\\" from the terminal')\n",
    "\n",
    "# task_dir = utils.io.train_dir_name(np.load(dataset_path, allow_pickle=True),\n",
    "#                              n_train,\n",
    "#                              use_sym=True,\n",
    "#                              use_cprsn=False,\n",
    "#                              use_E=True,\n",
    "#                              use_E_cstr=False)\n",
    "\n",
    "# !{create(dataset_path, n_train, 10, task_dir=task_dir)}\n",
    "# !{train(task_dir, dataset_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sgdml import cli\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def train(task_dir_arg, valid_dataset_path, overwrite=True, use_torch=False, max_processes=1):\n",
    "    valid_dataset = (valid_dataset_path, np.load(valid_dataset_path, allow_pickle=True))\n",
    "    model_dir_or_file_path = cli.train(task_dir_arg,\n",
    "                                       valid_dataset,\n",
    "                                       overwrite,\n",
    "                                       use_torch=use_torch,\n",
    "                                       max_processes=max_processes)\n",
    "    return np.load(model_dir_or_file_path, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sgdml import cli\n",
    "from sgdml import utils\n",
    "import gc\n",
    "gc.collect()\n",
    "global glob\n",
    "\n",
    "if 'glob' in globals():\n",
    "    del glob\n",
    "    \n",
    "n_train = 20\n",
    "n_valid = 20\n",
    "sigs = np.arange(1, 10)\n",
    "valid_dataset_path = dataset_path\n",
    "use_cg = False\n",
    "\n",
    "task_dir = utils.io.train_dir_name(np.load(dataset_path, allow_pickle=True),\n",
    "                             n_train,\n",
    "                             use_sym=True,\n",
    "                             use_cprsn=False,\n",
    "                             use_E=True,\n",
    "                             use_E_cstr=False)\n",
    "# \n",
    "\n",
    "dataset = (dataset_path, np.load(dataset_path, allow_pickle=True))\n",
    "if valid_dataset_path is None:\n",
    "    valid_dataset = dataset\n",
    "else:\n",
    "    valid_dataset = (valid_dataset_path, np.load(valid_dataset_path, allow_pickle=True))\n",
    "solver = 'cg' if use_cg else 'analytic'\n",
    "task_dir = cli.create(dataset,\n",
    "                      valid_dataset,\n",
    "                      n_train,\n",
    "                      n_valid,\n",
    "                      sigs,\n",
    "                      gdml = True,\n",
    "                      use_E= True,\n",
    "                      use_E_cstr = False,\n",
    "                      use_cprsn = False,\n",
    "                      overwrite = True,\n",
    "                      max_processes = None,\n",
    "                      task_dir=task_dir,\n",
    "                      solver=solver)\n",
    "task_dir_arg = cli.io.is_dir_with_file_type(task_dir, 'task', or_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glob' in globals():\n",
    "    del glob\n",
    "train(task_dir_arg, valid_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sgdml.predict import GDMLPredict\n",
    "from sgdml.utils import io\n",
    "import ase\n",
    "\n",
    "model = np.load('moldyn-unknown-train100-sym1.npz')\n",
    "ase.io.write('my_lattice.xyz', atoms)\n",
    "gdml = GDMLPredict(model)\n",
    "\n",
    "r,_ = io.read_xyz('my_lattice.xyz') # 4 atoms\n",
    "e,f = gdml.predict(r)\n",
    "\n",
    "print(r.shape) \n",
    "print(e.shape) \n",
    "print(f.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sgdml.predict import GDMLPredict\n",
    "from sgdml.utils import io\n",
    "import ase\n",
    "def plot_gdml(model, traj, start= 1000, every=10):\n",
    "    gdml = GDMLPredict(model)\n",
    "    y, x = [], []\n",
    "    y_test, x_test = [], []\n",
    "    for i, config in enumerate(traj):\n",
    "        if i % every == 0 and i > start:\n",
    "            if i <= (len(traj) + start)/2:\n",
    "                ase.io.write('temp.xyz', atoms)\n",
    "                r,_ = io.read_xyz('my_lattice.xyz')\n",
    "                y.append(config.get_forces().reshape([1, -1]))\n",
    "                x.append(config.get_positions().reshape([1, -1]))\n",
    "            else:\n",
    "                x_test.append(config.get_positions().reshape([1, -1]))\n",
    "                y_test.append(config.get_forces().reshape([1, -1]))\n",
    "\n",
    "    \n",
    "    y_hat_train = gpr.predict(x)\n",
    "    y_hat_train_norm = np.linalg.norm(y_hat_train, axis=1)\n",
    "    y_train_norm = np.linalg.norm(y, axis=1)\n",
    "    y_hat = gpr.predict(x_test)\n",
    "    y_hat_norm = np.linalg.norm(y_hat, axis=1)\n",
    "    y_test_norm = np.linalg.norm(y_test, axis=1)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, axis = plt.subplots(ncols=1, nrows=2, sharex=True)\n",
    "    x1 = np.arange(len(y_hat_train_norm))\n",
    "    axis[0].plot(x1, y_hat_train_norm, label='predicted on train')\n",
    "    axis[0].plot(x1, y_train_norm, label='training')\n",
    "    x2 = np.arange(len(y_hat_norm))+len(y_hat_train_norm)\n",
    "    axis[0].plot(x2, y_hat_norm, label='predicted')\n",
    "    axis[0].plot(x2, y_test_norm, label='ground truth')\n",
    "    axis[0].set_xticks([])\n",
    "    axis[0].set_title('The force norm')\n",
    "    axis[0].legend()\n",
    "\n",
    "    x1 = np.arange(len(y_hat_train_norm))\n",
    "    axis[1].plot(x1, y_hat_train_norm-y_train_norm, label='diffrence on train')\n",
    "    x2 = np.arange(len(y_hat_norm))+len(y_hat_train_norm)\n",
    "    axis[1].plot(x2, y_hat_norm-y_test_norm, label='diffrence on test')\n",
    "    axis[1].legend()\n",
    "    plt.show()\n",
    "    \n",
    "traj = Trajectory('moldyn.traj', 'r', atoms)\n",
    "start, every = 1000, 10\n",
    "plot_gdml(model, traj, start= 1000, every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgdml.intf.ase_calc import SGDMLCalculator\n",
    "\n",
    "from ase.io import read\n",
    "from ase.optimize import QuasiNewton\n",
    "from ase.md.velocitydistribution import (MaxwellBoltzmannDistribution, Stationary, ZeroRotation)\n",
    "from ase.md.verlet import VelocityVerlet\n",
    "from ase import units\n",
    "\n",
    "model_path = 'moldyn-unknown-train100-sym1.npz'\n",
    "calc = SGDMLCalculator(model_path)\n",
    "\n",
    "mol = atoms\n",
    "mol.set_calculator(calc)\n",
    "\n",
    "# do a quick geometry relaxation\n",
    "# qn = QuasiNewton(mol)\n",
    "# qn.run(1e-4, 100)\n",
    "\n",
    "# set the momenta corresponding to T=300K\n",
    "MaxwellBoltzmannDistribution(mol, 300 * units.kB)\n",
    "Stationary(mol) # zero linear momentum\n",
    "ZeroRotation(mol) # zero angular momentum\n",
    "\n",
    "# run MD with constant energy using the VelocityVerlet algorithm\n",
    "dyn = Langevin(mol, 5 * units.fs, T * units.kB, 0.002, trajectory='md.traj')\n",
    "# dyn = VelocityVerlet(mol, 0.2 * units.fs)  # 0.2 fs time step.\n",
    "\n",
    "t0 = time.time()\n",
    "def printenergy(a=atoms):  # store a reference to atoms in the definition.\n",
    "    \"\"\"Function to print the potential, kinetic and total energy.\"\"\"\n",
    "    epot = a.get_potential_energy() / len(a)\n",
    "    ekin = a.get_kinetic_energy() / len(a)\n",
    "    print('Energy per atom: Epot = %.3feV  Ekin = %.3feV (T=%3.0fK)  '\n",
    "          'Etot = %.3feV, time elapsed:%3.0f' % (epot, ekin, ekin / (1.5 * units.kB), epot + ekin, time.time()-t0))\n",
    "    \n",
    "dyn.attach(printenergy, interval=100)\n",
    "\n",
    "# now run the dynamics\n",
    "printenergy(mol)\n",
    "dyn.run(100000)\n",
    "try:\n",
    "    del calc\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For scalar output\n",
    "$$\n",
    "\\begin{bmatrix}f^*\\end{bmatrix}^{1\\times1} = \\begin{bmatrix}{k^*}^T\\end{bmatrix}^{1\\times M}\\begin{bmatrix}\\begin{bmatrix}K^{-1}\\end{bmatrix}^{M \\times M}\\begin{bmatrix}y\\end{bmatrix}^{M\\times 1}\\end{bmatrix}^{M\\times1}=\\sum_{i}^{M}\\alpha_i k(x_i,x^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For vector output\n",
    "$$\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} ={k^*}^{T}K^{-1}\\mathbf{f}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j}\\nabla k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K=\\nabla_{x} k(x,x') \\nabla_{x^{'}}^\\top = \n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\\nabla_{x^{(1)}} k(x^{(1)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} & \\dots & \\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} \\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} & \\dots & \\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(M)}) \\nabla_{x^{(M)}}^\\top\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\nabla k(x^{(i)}, x^{(j)})\\nabla ^\\top=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{(j)}_1 }\\nabla k(x^{(i)},x^{(j)}),\\dots,\\frac{\\partial}{\\partial x^{(j)}_{3N} }\\nabla k(x^{(i)},x^{(j)})\\end{bmatrix}^{3N\\times 3N}\n",
    "=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{(j)}_1 }\\frac{\\partial}{\\partial x^{(i)}_{1}} k(x^{(i)},x^{(j)}) &\\dots & \\frac{\\partial}{\\partial x^{(j)}_{3N} }\\frac{\\partial}{\\partial x^{(i)}_{1}} k(x^{(i)},x^{(j)}) \\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\frac{\\partial}{\\partial x^{(j)}_1}\\frac{\\partial}{\\partial x^{(i)}_{3N}} k(x^{(i)},x^{(j)}) &\\dots & \\frac{\\partial}{\\partial x^{(j)}_{3N} }\\frac{\\partial}{\\partial x^{(i)}_{3N}} k(x^{(i)},x^{(j)})\n",
    "\\end{bmatrix}^{3N\\times 3N}\n",
    "\\\\\n",
    "\\begin{bmatrix}K\\end{bmatrix}^{M\\cdot3N\\times M\\cdot3N} \\;\n",
    "\\begin{bmatrix}\\mathbf{f}\\end{bmatrix}^{M\\cdot3N\\times 1}\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "{k^*}^{T}\n",
    "=\\nabla_x k(x^{*}, x)\\nabla_{x^{'}} ^\\top \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{pmatrix}\\nabla_{x^*} k(x^{*}, x^{(1)})\\nabla_{x^{(1)}} ^\\top\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\nabla k(x^{*}, x^{(M)})\\nabla_{x^{(M)}} ^\\top\\end{pmatrix}\n",
    "\\end{bmatrix}^{3N\\times M\\cdot 3N}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(1)}_1 }\\nabla k(x^{*},x^{(1)}),\\dots,\\frac{\\partial}{\\partial x^{(1)}_{3N} }\\nabla k(x^{*},x^{(1)})\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(M)}_1 }\\nabla k(x^{*},x^{(M)}),\\dots,\\frac{\\partial}{\\partial x^{(M)}_{3N} }\\nabla k(x^{*},x^{(M)})\\end{pmatrix}\n",
    "\\end{bmatrix}^{3N\\times M\\cdot 3N}\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} =\\begin{bmatrix}\\begin{bmatrix}{k^*}^{T}\\end{bmatrix}^{3N \\times M \\cdot 3N}\\begin{bmatrix}\\begin{bmatrix}K^{-1}\\end{bmatrix}^{M\\cdot 3N \\times M\\cdot 3N}\\begin{bmatrix}\\mathbf{f}\\end{bmatrix}^{M\\cdot 3N \\times 1}\\end{bmatrix}^{M \\cdot 3N \\times 1}\\end{bmatrix}^{3N\\times 1}\n",
    "\\\\\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} =\n",
    "{k^*}^{T}\n",
    "K^{-1}\n",
    "\\mathbf{f}=\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(1)}_1 }\\nabla k(x^{*},x^{(1)}),\\dots,\\frac{\\partial}{\\partial x^{(1)}_{3N} }\\nabla k(x^{*},x^{(1)})\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(M)}_1 }\\nabla k(x^{*},x^{(M)}),\\dots,\\frac{\\partial}{\\partial x^{(M)}_{3N} }\\nabla k(x^{*},x^{(M)})\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\underbrace{ K^{-1}\\mathbf{f}}_{\\alpha _{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matérn kernel\n",
    "For our application, we considered a subclass from the parametric\n",
    "Matérn family (22–24) of (isotropic) kernel functions\n",
    "$$\n",
    "k: C_{\\nu=n+\\frac{1}{2}}(d)=exp{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}P_{n}(d)\n",
    "$$\n",
    "$$\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$\n",
    "\n",
    "### The full kernel\n",
    "\n",
    "$$\n",
    "\\mathbf{k_F(x, x^{'})}=\\nabla k(x, x^{'})\\nabla ^\\top=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{'}_1 }\\nabla k(x,x^{'}),\\dots,\\frac{\\partial}{\\partial x^{'}_{3N} }\\nabla k(x,x^{'})\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "=\\begin{pmatrix}5(\\mathbf{x-x^{'}})(\\mathbf{x-x^{'}})^\\top-\\mathbb{1}\\sigma(\\sigma +\\sqrt{5}d))\\end{pmatrix}\\frac{5}{3\\sigma^{4}}exp\\begin{pmatrix}-\\frac{\\sqrt{5}d}{\\sigma}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k_F(x, x^{'})}\\in \\mathbb{R}^{3N \\times 3N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k_E(x, x^{'})}=k(x, x^{'})\\nabla ^\\top\n",
    "=5(\\mathbf{x-x^{'}})(\\sigma+d)\\frac{5}{3\\sigma^{3}}exp\\begin{pmatrix}-\\frac{\\sqrt{5}d}{\\sigma}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k_E(x, x^{'})}\\in \\mathbb{R}^{1 \\times 3N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matérn covariance derivatives\n",
    "$$\n",
    "k: C_{\\nu=n+\\frac{1}{2}}(d)=B(d)P_{n}(d)\n",
    "\\\\\n",
    "B(d) = exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "\\\\\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\kappa}{\\partial x_i} = \\frac{\\partial P_n}{\\partial x_i}B + \\frac{\\partial B}{\\partial x_i}P_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial P_n}{\\partial x_i}=\\sum_{k=0}^n{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\frac{(n-k)(x_i -x^{'}_i)}{d^2}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial B}{\\partial x_i} =-\\frac{\\sqrt{2\\nu}(x_i -x^{'}_i)}{\\sigma d}exp{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial^2 \\kappa}{\\partial x_i \\partial x_j} = \n",
    "B\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j} +\n",
    "\\frac{\\partial B}{\\partial x_i}\\frac{\\partial P_n}{\\partial x_j} +\n",
    "\\frac{\\partial B}{\\partial x_j}\\frac{\\partial P_n}{\\partial x_i} +\n",
    "P_n\\frac{\\partial^2 B}{\\partial x_i \\partial x_j} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i\\ne j}=\n",
    "\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n \\\\ k \\end{pmatrix}\n",
    "\\frac{(n-k-2)(n-k)(x_i -x^{'}_i)(x_j - x^{'}_j)}{d^4}\n",
    "\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i=j}=\n",
    "\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n \\\\ k \\end{pmatrix}\n",
    "\\frac{(n-k-2)(n-k)(x_i -x^{'}_i)^2}{d^4}\n",
    "\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}} + \\sum_{k=0}^n{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\frac{(n-k)}{d^2}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 B}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i\\ne j} =\n",
    "\\frac{\\sqrt{2\\nu}(x_i-x^{'})(x_j-x^{'}_j)(\\sqrt{2\\nu} d+\\sigma)}{\\sigma^2d^3}\\exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 B}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i = j} =\n",
    "\\frac{\\sqrt{2\\nu}(x_i-x^{'})(x_j-x^{'}_j)(\\sqrt{2\\nu} d+\\sigma)}{\\sigma^2d^3}\\exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}} - \\frac{\\sqrt{2\\nu}(x_i -x^{'}_i)}{\\sigma d}exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
