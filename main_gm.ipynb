{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n",
    "# !conda install nglview -c conda-forge -y\n",
    "# !conda install -c conda-forge kimpy openkim-models -y\n",
    "# !pip install ase\n",
    "# !conda install matplotlib -y\n",
    "# !conda install scipy -y\n",
    "# !conda install tqdm -y\n",
    "# !pip install nglview scikit-learn scipy ipympl ipywidgets matplotlib\n",
    "# !pip install sgdml\n",
    "# !pip install sgdml[ase]\n",
    "# !pip install sgdml[torch]\n",
    "# !jupyter-nbextension enable nglview --py --sys-prefix\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *\n",
    "import gc\n",
    "import os\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Driven Force Fields for Large Scale Molecular Dynamics Simulations of Halide Perovskites  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Halide Perovskites (HaPs) \n",
    "\n",
    "Halide perovskites (HaPs) are a class of materials with the general formula ABX3. HaPs achieve very\n",
    "high efficiency for solar cells in a very short amount of time while keeping relativity low manufacturing\n",
    "cost. Another added benefit of HaPs is the ability to manufacture them with a wide range of possible\n",
    "bandgap values, which can be used for many bandgap sensitive application, including light-emitting\n",
    "diodes, photocatalysis, and more.$^{1–5}$ \n",
    "\n",
    "<img src=\"https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20200701152913383-0351:S0883769420001402:S0883769420001402_fig1.png\" width=\"700\">\n",
    "\n",
    "HaPs are a convergence point of many interesting structural properties. Including the ability to occupy\n",
    "the halide occupied X site with a large alkali atom or a small organic molecule, which produces hybrid\n",
    "organic-inorganic materials. HaPs possess long-range crystalline order, but several phases of HaPs may\n",
    "be present in one material. HaPs are much softer and have lower optical phonon energies compare to\n",
    "typical inorganic semiconductors. Pb-based HaPs are dynamically disordered at RT, and more. Each of\n",
    "these properties is not unprecedented, but the convergence of these properties in one material is\n",
    "peculiar.\n",
    "$^{4}$\n",
    "\n",
    "\n",
    "\n",
    "## Theoretical Research of the Dynamics of Halide Perovskite \n",
    "\n",
    "Many of the properties of the HaPs are based on the unique structure and dynamics of the HaPs system. Also, numerous reports show that, even without defects, HaPs shows significant structural fluctuations.$^{6}$ One of the main methods of investigating the properties of HaPs theoretically is molecular dynamics (MD). By employing the MD method one can obtain extensive dynamical information - including the actual location of the atoms, energy levels and the band structure, dipoles, and much more. \n",
    "\n",
    "## Molecular Dynamics (MD) \n",
    "\n",
    "Molecular Dynamics (MD) simulation is an algorithm for computing equilibrium and transport properties of many-body systems at a finite temperature. This method relies on the Newtonian equations of motion, updated each step numerically according to calculated forces. The applied forces of all the particles/atoms in the systems are updated by various possible methods, a few of which are discussed below.$^{7}$  \n",
    "<img src=\"Images/MD.png\" />\n",
    "\n",
    "### Classical Potential Molecular Dynamics \n",
    "\n",
    "Most common MD simulations are based on classical potentials. These potentials are usually based on principles of quantum mechanics but only in the form of fitted parameters, while the implementation of the equations is classical – atoms are regarded as points in space. The potential parameters need to be determined empirically for each set of materials. They are therefore limited in accuracy and types of systems.  \n",
    "\n",
    "$$\n",
    "U=\\sum_{i=1}^{N}{v_1(\\mathbf{r_i})}\\; +\\; \\sum_{i=1}^{N}{\\sum_{j=1}^{N}{\\phi_{i, j}(\\mathbf{r_i, r_j})}}\n",
    "\\; +\\; \\sum_{i=1}^{N}{\\sum_{j=1}^{N}{\\sum_{k=1}^{N}{v_{3}(\\mathbf{r_i, r_j, r_k})}}}\n",
    "$$\n",
    "\n",
    "### First-Principles Molecular Dynamics \n",
    "\n",
    "First-principles molecular dynamics are dynamical simulations, in which forces on the atoms are calculated using first-principles methods. By far the most common first-principles method is density functional theory (DFT). First-principles calculations are considered more accurate than classical potentials and inherently take into account the quantum nature of the interaction of the atoms and the electron cloud. Their critical limitation, however, is the computational cost of a meaningful simulation. While it is possible to obtain many useful insights from those simulations, for HaPs, which are highly dynamical materials, these limitations are hard to surpass.\n",
    "\n",
    "$$\n",
    "E_{tot}= -\\frac{\\hbar^2}{2m}\\sum_{i}{\\langle \\phi_i | \\nabla^2 | \\phi_i\\rangle} \\;+\\;\n",
    "\\int v_{ext}(\\vec{r})n(\\vec{r})d^3r \\;+\\;\n",
    "\\frac{e^2}{2}\\int{\\int{\\frac{n(\\vec{r_1})n(\\vec{r_2})}{|\\vec{r_1}-\\vec{r_2}|}d^3\\vec{r_1}}d^3{\\vec{r_2}}} \\;+\\;\n",
    "E_{xc}[n]\n",
    "$$\n",
    "\n",
    "## Molecular Dynamics of Halide Perovskite \n",
    "\n",
    "HaPs are comprised of at least three or more types of atoms and have many anharmonic features. This makes the usage of classical potential MD simulation very difficult for those materials. The only current possible option for dynamical research is of use first-principles MD, which limit the scale of the simulations. \n",
    "\n",
    "## Data-Driven Molecular Dynamics  \n",
    "\n",
    "Until recently, constructing MD potentials was based on benchmark data and a small set of first-principles calculations, used to fit a classical MD potential. This process is highly specialized and involved, and required much understanding of the modeled system.\n",
    "\n",
    "<img src=\"https://pubs.rsc.org/en/Image/Get?imageInfo.ImageType=GA&imageInfo.ImageIdentifier.ManuscriptID=C6SC05720A&imageInfo.ImageIdentifier.Year=2017\">\n",
    "\n",
    "The ever-increasing processing power and the rise of shared experimental and computational results data repositories provide the opportunity to easily obtaining a vast amount of information and data for each desired system. This change of setting allows the emergence of data-driven MD potentials. Those potentials are based on statistical learning, a branch of mathematics looking to extract and predict new properties from a given dataset.8  These approaches rely on much larger amounts of data compares to the classical approach. However, obtaining the potential is more general and can be easily applied to different systems. \n",
    "\n",
    "The two most common approaches are the artificial neural network (ANN) and the Gaussian process (GP). The methods are mathematical, but can be enhanced by using physical insights.9,10 The ANN is considered by many as a \"black box\" because the model has low interpretability but, by definition, can approximate any continuous function to a desired accuracy with enough data and a large enough network.11 GP's are somewhat limited in their prediction limits, but they are much easier to understand. Thus, it is possible to add deep physical insights that can drastically decrease the needed dataset size for the same accuracy.\n",
    "\n",
    "<img src=\"https://www.tandfonline.com/na101/home/literatum/publisher/tandf/journals/content/tapx20/2019/tapx20.v004.i01/23746149.2019.1654919/20200118/images/medium/tapx_a_1654919_f0001_oc.jpg\">\n",
    "\n",
    "\n",
    "## Gaussian Process Force-Field \n",
    "This work proposed to use the symmetrized gradient-domain machine learning (sGDML) method, a Gaussian process-based force field. The GP method is based on jointly gaussian distributions. Each configuration of the system is considered a single gaussian distribution coupled in an approximate manner to other configurations, by using a predefined \"kernel function\" which is considered a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Normal/Gaussian distribution\n",
    "Normal distributions are important in statistics and are often used to represent real-valued random variables whose distributions are not known. Their importance is partly due to the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. \n",
    "\n",
    "based on: https://peterroelants.github.io/posts/multivariate-normal-primer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Univariate normal distribution\n",
    "\n",
    "Univariate/single normal distribution is the simple case that the for given single value-scaler we have continuous probability distribution.\n",
    "$$\n",
    "\\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_normal(x, mean, variance):\n",
    "    \"\"\"pdf of the univariate normal distribution.\"\"\"\n",
    "    return ((1. / np.sqrt(2 * np.pi * variance)) * \n",
    "            np.exp(-((x - mean)**2) / (2 * variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(x, mean, variance):\n",
    "    return ((1. / (2 * variance) * \n",
    "            np.exp(-np.abs(x - mean) / ( variance))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fermi(x, mean, variance):\n",
    "    return 1/(np.exp((x-mean)/variance)+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Boltzmann(x, mean, variance):\n",
    "    return 0.797*((x**2)*np.exp((-x**2)/(2*variance**2)))/variance**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dirac(x, mean, variance):\n",
    "    pdf = np.zeros(x.shape)\n",
    "    pdf[x==mean] = 1\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential(x, mean, variance):\n",
    "    pdf = (1/mean)*np.exp((1/mean)*-x)\n",
    "    pdf[x<0] = 0\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "univariate_plot(univariate_normal)\n",
    "#univariate_plot(laplace, normal=False)\n",
    "#univariate_plot(fermi, normal=False)\n",
    "#univariate_plot(Boltzmann, normal=False, xlim=[0, 5])\n",
    "#univariate_plot(exponential, normal=False, xlim=[0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Multivariate normal distribution\n",
    "multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d \\lvert\\Sigma\\rvert}} \\exp{ \\left( -\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{\\mu}, \\Sigma) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2\\\\ \\dots\\\\ x_n\\end{pmatrix},\\; \n",
    "\\mathbf{\\mu} = \\begin{pmatrix} \\mu_1 \\\\ \\mu_2\\\\ \\dots\\\\ \\mu_n\\end{pmatrix}, \n",
    "\\mathbf{\\Sigma} = \\begin{pmatrix}    \\Sigma_{11} & \\Sigma_{12} &\\dots & \\Sigma_{1n}\n",
    "                                  \\\\ \\Sigma_{21} & \\Sigma_{22} & \\dots & \\Sigma_{2n}\n",
    "                                  \\\\ \\dots & \\dots & \\dots & \\dots\n",
    "                                  \\\\ \\Sigma_{n1} & \\dots & \\dots &\\Sigma_{nn}\\end{pmatrix}, \\mathbf{\\Sigma} =\\mathbf{\\Sigma^\\top}: \\Sigma_{21}=\\Sigma_{21}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal(x, d, mean, covariance):\n",
    "    \"\"\"pdf of the multivariate normal distribution.\"\"\"\n",
    "    x_m = x - mean\n",
    "    return (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) * \n",
    "            np.exp(-(x_m.T.dot(np.linalg.inv(covariance).dot(x_m))) / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 2D Multivariate normal distribution\n",
    "\n",
    "$$\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, \n",
    "\\begin{bmatrix}\n",
    "1 & C \\\\\n",
    "C & 1\n",
    "\\end{bmatrix}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "multivariate_plot(multivariate_normal, nb_of_x=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Conditional normal distributions\n",
    "\n",
    "If both $\\mathbf{y_1}$ and $\\mathbf{y_2}$ are [jointly normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Joint_normality) the random vectors defined as:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{y_1} \\\\\n",
    "\\mathbf{y_2} \n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "\\mu_{\\mathbf{1}} \\\\\n",
    "\\mu_{\\mathbf{2}}\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\ \n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "= \\mathcal{N}(\\mu, \\Sigma)\n",
    ", \\qquad \n",
    "$$\n",
    "\n",
    "The [conditional distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution) of $\\mathbf{y_1}$ given $\\mathbf{y_2}$ is defined as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y_1} \\mid \\mathbf{y_2}) = \\mathcal{N}(\\mu_{1|2}, \\Sigma_{1|2})\n",
    "$$\n",
    "\n",
    "With:\n",
    "$$\\mu_{1|2} = \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2) ,\\;\n",
    "\\Sigma_{1|2} = (\\Sigma/\\Sigma_{22})=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see [Proof 1](#Proof_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "condition_plot(nb_of_x=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Gaussian Process \n",
    "Gaussian process (GP) Is a method predicting $y^*$ for a given $x^*$ and getting: $y_i=f(x_i )$. \n",
    "GP assumes that $p(f(x_1),...,f(x_N  ))$ is a jointly Gaussian, i.e., the probability of getting the full random vector (the known $y$ points and the the new $y^{*}$ points) is defined as a multidimensional gaussian with $\\mu$ and $\\Sigma(x)$, where $\\Sigma(x)$ is calculated as $\\Sigma_{ij}=k(x_i,x_j)$, and $k$ is a kernel function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y^*=f^*(x^*; \\mathbf{x}, \\mathbf{y}) \\sim\n",
    "\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{\\mu(x^*; \\mathbf{x}, \\mathbf{y})}, \\;\n",
    "\\mathbf{\\Sigma(x^*; \\mathbf{x}, \\mathbf{y})}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{y} \\\\ \n",
    "y^*\n",
    "\\end{pmatrix}\\Rightarrow\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{f} \\\\ \n",
    "f^*\n",
    "\\end{pmatrix} \\sim\n",
    "\\mathcal{N}\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{\\mu(x, y)} \\\\ \n",
    "\\mu^*(x^*)\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{K(x,x)} & \\mathbf{k(x^*,x)^*}\\\\ \n",
    "{\\mathbf{k(x^*, x)^{*}}}^{\\top} & {k(x^*,x^*)^{**}}\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "<br/>\n",
    "$\\mathbf{f}$ - Vector of all observed $y_i$ values, $f=y_i=f(x_i)$.<br/>\n",
    "$f^*$ - Prediction function for point $x^*$, $f^*=y^*=f(x^*)$.<br/>\n",
    "$\\mathbf{\\mu}$ - Vector of all observed mean values.<br/>\n",
    "$\\mathbf{\\mu^*}$- Mean values for the prediction for x^*. <br/>\n",
    "$\\mathbf{K}$ - Covariance matrix of all observed points<br/>\n",
    "$\\mathbf{k^*}$ - Kernel vector $k^*=k(x^*,x_i )$<br/>\n",
    "${k^{**}}$ - Self kernel $k^{**}=k(x^*,x^* )$<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Applying the Conditional \n",
    "$$f(x^*)=f(x^*|\\mathbf{y},\\mathbf{x})=\\mu(x^*|\\mathbf{y},\\mathbf{x})=\\mu(x^*)^* +{{\\mathbf{k}}^{*}}^{\\top}  \\mathbf{K}^{-1}(\\mathbf{y}-\\mu(x, y))$$ <br/>\n",
    "For simplicity, we can define the mean values to be zero<br/>\n",
    "$$f(x^*) ={\\mathbf{k^*}}^{\\top}\\mathbf{K}^{-1}\\mathbf{y}=\\sum_{i=1}^{N}\\alpha_ik(x_i, x^*)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 The Conditional on A Nosey Dataset\n",
    "If we have a training dataset with noisy function values $\\mathbf{y} = \\mathbf{f} + \\boldsymbol\\epsilon$ where noise $\\boldsymbol\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_y^2 \\mathbf{I})$ is independently added to each observation then the predictive distribution is given by\n",
    "\n",
    "$$\n",
    "f(x^*) ={\\mathbf{k^*}}^{\\top}\\mathbf{K_y}^{-1}\\mathbf{y}=\\sum_{i=1}^{N}\\alpha_ik(x_i, x^*)\n",
    "$$\n",
    "where $\\mathbf{K}_y = \\mathbf{K} + \\sigma_y^2\\mathbf{I}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kernel Function\n",
    "the is the kernel function which can be regarded as a \"distance\" function which defines how strongly the value $f(x_i)$ is coupled to point $f(x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 The Most Popular Kernal\n",
    "In this part we will use the squared exponential kernel, also known as Gaussian kernel or radial basis function (RBF) kernel. which is the go to kernel for starting the GP regression.\n",
    "$$\n",
    "\\kappa(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2}\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)\\right)\\tag{10}\n",
    "$$\n",
    "\n",
    "Based on:\\\n",
    "https://juanitorduz.github.io/gaussian_process_reg/ \\\n",
    "http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "\n",
    "Kernels:\n",
    "https://data-flair.training/blogs/svm-kernel-functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes\n",
    "    a covariance matrix from points in X1 and X2.\n",
    "\n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    def k(x1, x2):\n",
    "        return np.linalg.norm(x1-x2)**2\n",
    "        \n",
    "    m, n = X1.shape[0], X2.shape[0]\n",
    "    sqdist = np.array([[k(X1[i], X2[j]) for j in range(n)] for i in range(m)])\n",
    "    return sigma_f ** 2 * np.exp(-0.5 / l ** 2 * sqdist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 More kernals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Laplace_RBF(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    '''\n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    def k(x1, x2):\n",
    "        return np.linalg.norm(x1-x2)\n",
    "        \n",
    "    m, n = X1.shape[0], X2.shape[0]\n",
    "    sqdist = np.array([[k(X1[i], X2[j]) for j in range(n)] for i in range(m)])\n",
    "    return sigma_f ** 2 * np.exp(-0.5 / l ** 2 * sqdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(X1, X2, d=1.0):\n",
    "    '''\n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    def k(x1, x2):\n",
    "        return (x1.dot(x2)+1)**d\n",
    "        \n",
    "    m, n = X1.shape[0], X2.shape[0]\n",
    "    return np.array([[k(X1[i], X2[j]) for j in range(n)] for i in range(m)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Deeper look into the calcaulted covirnce matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "n_sample = 100\n",
    "n_predict = 0\n",
    "\n",
    "x = plot_noise_sin(n_sample, plot_line=True, label='samples')\n",
    "x_add = plot_noise_sin(n_predict, label='prediction')\n",
    "\n",
    "x = np.concatenate([x, x_add], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "plt.close('all')\n",
    "heapmap_kernal(x.reshape([-1, 1]),  partial(RBF, l=1.0, sigma_f=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implementation of the Conditional\n",
    "$$\n",
    "f(x^*) ={\\mathbf{k^*}}^{\\top}\\mathbf{K_y}^{-1}\\mathbf{y}=\\sum_{i=1}^{N}\\alpha_ik(x_i, x^*)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_predictive(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8, kernal=RBF):\n",
    "    '''\n",
    "    Computes the suffifient statistics of the GP posterior predictive distribution\n",
    "    from m training data X_train and Y_train and n new inputs X_s.\n",
    "\n",
    "    Args:\n",
    "        X_s: New input locations (n x d).\n",
    "        X_train: Training locations (m x d).\n",
    "        Y_train: Training targets (m x 1).\n",
    "        l: Kernel length parameter.\n",
    "        sigma_f: Kernel vertical variation parameter.\n",
    "        sigma_y: Noise parameter.\n",
    "\n",
    "    Returns:\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\n",
    "    '''\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y ** 2 * np.eye(len(X_train))\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) \n",
    "    K_inv = inv(K)\n",
    "\n",
    "\n",
    "    alphas = K_inv.dot(Y_train)\n",
    "    mu_s = K_s.T.dot(alphas)\n",
    "\n",
    "\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "\n",
    "    return mu_s, cov_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "noise = 0.1\n",
    "x = [-3, -2 , -1,  1, 2 , 3, 4]\n",
    "def f(x):\n",
    "    func = np.exp(np.cos(x))\n",
    "    return func\n",
    "    \n",
    "gaussian_process(x, f, noise, posterior_predictive, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "noise = 0.3\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
    "kernal = ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "\n",
    "gaussian_process_interactive(f, noise, kernal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 GP Force Field \n",
    "\n",
    "A straightforward formulation of a vector-valued estimator\n",
    "takes the form:\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x), \\dots, \\hat{f}_N(x)\\end{bmatrix}^{T}\n",
    "$$\n",
    "$\\mathbf{\\hat{f}}: \\mathbb{R}^N\\rightarrow\\mathbb{R}^N$ where each component: ${\\hat{f}_i}: \\mathbb{R}^N\\rightarrow\\mathbb{R}$\n",
    "\n",
    "And it easy to applied it directly for prediction the forces on $N$ atoms system with $N$ 3D position vector and $N$ 3D forces vectors:\n",
    "$\\mathbf{\\hat{f}}: \\mathbb{R}^{3N}\\rightarrow\\mathbb{R}^{3N}$ where each component: ${\\hat{f}_i}: \\mathbb{R}^{3N}\\rightarrow\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "from ase_utils import *\n",
    "from ase import Atom, Atoms\n",
    "from ase.visualize import ngl\n",
    "\n",
    "\n",
    "a = 3.519\n",
    "atoms_type = 'Ni'\n",
    "# https://wiki.fysik.dtu.dk/ase/ase/atoms.html#module-ase.atoms\n",
    "atoms = Atoms([Atom(atoms_type, (0, 0, 0)), \n",
    "               Atom(atoms_type, (0, a/2, a/2)),\n",
    "               Atom(atoms_type, (a/2, 0, a/2)),\n",
    "               Atom(atoms_type, (a/2, a/2, 0))])\n",
    "atoms.set_cell([a, a, a])\n",
    "atoms.set_pbc(True)\n",
    "atoms = atoms*2\n",
    "atoms_copy = atoms.copy()\n",
    "lat = np.array(atoms.get_cell())\n",
    "\n",
    "ngl.view_ngl(atoms, w=500, h=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge kimpy openkim-models -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.calculators.emt import EMT\n",
    "from ase.calculators.kim.kim import KIM\n",
    "T = 300  # Kelvin\n",
    "# Use openKIM calculator for easy of use: https://openkim.org/\n",
    "# Mishin Ni99\n",
    "atoms.calc = KIM(\"EAM_Dynamo_MishinFarkasMehl_1999_Ni__MO_400591584784_005\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution\n",
    "from ase.md.langevin import Langevin\n",
    "# Set the momenta corresponding T\n",
    "MaxwellBoltzmannDistribution(atoms, 2*T*units.kB)\n",
    "# Two times room temperature simulation, Langevin thermostat\n",
    "dyn = Langevin(atoms, 10 * units.fs, 2*T*units.kB, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import Trajectory\n",
    "# Now run the dynamics\n",
    "steps = 10000\n",
    "interval = 1\n",
    "\n",
    "dyn.attach(printenergy(atoms, steps, interval), interval=interval)\n",
    "# We also want to save the positions of all atoms after every interval time step.\n",
    "#os.remove('reference.traj') # if redoing to MD simulation please delete the previous file\n",
    "traj = Trajectory('reference.traj', 'w', atoms)\n",
    "dyn.attach(traj.write, interval=interval)\n",
    "\n",
    "dyn.run(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "gc.collect()\n",
    "traj = Trajectory('reference.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The Vector-Valued Estimator Problem\n",
    "When mapping a scalar function $\\mathbb{R}^{N}\\rightarrow\\mathbb{R}$ each entry in the kernel function correspond to one input vector ($x_i$). In the vector output case, we have multiple outputs for each input. Because of this limitation, we need to construct our kernel function carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 The Navie Approch:\n",
    "We can create an independent estimator for each value in the force vector.\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x), \\dots, \\hat{f}_N\\end{bmatrix}^{T}\\\\\n",
    "$$\n",
    "$$\n",
    "\\hat{f}_i(x) = \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_i}, \\mathbf{k(x,x^{'})_i}\\end{bmatrix}\\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{\\hat{f}}=\\begin{bmatrix}\\hat{f}_1(x) \\\\ \\dots \\\\ \\hat{f}_N(x)\\end{bmatrix}\n",
    "=\\begin{bmatrix}\\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_1}, \\mathbf{k(x,x^{'})_1}\\end{bmatrix} \\\\ \n",
    "\\dots\n",
    "\\\\ \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\mu(x)_N}, \\mathbf{k(x,x^{'})_N}\\end{bmatrix}\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = Trajectory('reference.traj', 'r', atoms)\n",
    "start, every = 100, 10\n",
    "(x, y), (x_test, y_test) = md_dataset_split(traj, start, every)\n",
    "print(f'training points: {len(x)}')\n",
    "steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "noise = 0.1\n",
    "plt.close('all')\n",
    "kernal= ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "atoms = atoms_copy.copy()\n",
    "atoms.calc = GPRCalculator(gpr)\n",
    "MaxwellBoltzmannDistribution(atoms, T * 2 * units.kB)\n",
    "dyn = Langevin(atoms, 1 * units.fs, T * units.kB, 0.002)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'w', atoms)\n",
    "interval = 1\n",
    "dyn.attach(printenergy_slim(steps, interval), interval=interval)\n",
    "dyn.attach(traj.write, interval=10)\n",
    "dyn.run(steps)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Some Tricks\n",
    "##### Changing the kernal:\n",
    "https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes\n",
    "##### The Matérn kernel\n",
    "It is generally assumed that overly smooth priors are detrimental to data efficiency, even if the prediction target is in fact indefinitely differentiable. For that reason, we will try to uses a Matérn kernel with restricted differentiability to construct the force field kernel function,\n",
    "$$\n",
    "k: C_{\\mu=n+\\frac{1}{2}}(d)=exp\\begin{pmatrix}{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}P_{n}(d)\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
    "gc.collect()\n",
    "plt.close('all')\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "atoms = atoms_copy.copy()\n",
    "atoms.calc = GPRCalculator(gpr)\n",
    "MaxwellBoltzmannDistribution(atoms, T * 2 * units.kB)\n",
    "dyn = Langevin(atoms, 1 * units.fs, T * units.kB, 0.002)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'w', atoms)\n",
    "interval = 1\n",
    "dyn.attach(printenergy_slim(steps, interval), interval=interval)\n",
    "dyn.attach(traj.write, interval=10)\n",
    "dyn.run(steps)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize y\n",
    "By normalizing the output value, we can \"center\" our output values around zero and $\\sigma=1$ for all dimensions and reduce the overexpression of a certain axis.\n",
    "$$\n",
    "\\hat{y}_{train} = \\frac{y_{train}-E[y_{train}]}{\\sqrt{Var[y_{train}]}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2, normalize_y=True)\n",
    "gpr.fit(x, y)\n",
    "plot_gpr(gpr, x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "atoms = atoms_copy.copy()\n",
    "atoms.calc = GPRCalculator(gpr)\n",
    "MaxwellBoltzmannDistribution(atoms, T * 2 * units.kB)\n",
    "dyn = Langevin(atoms, 1 * units.fs, T * units.kB, 0.002)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'w', atoms)\n",
    "interval = 1\n",
    "dyn.attach(printenergy_slim(steps, interval), interval=interval)\n",
    "dyn.attach(traj.write, interval=10)\n",
    "dyn.run(steps)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Roto-translational invariance\n",
    "Using cartesian coordinate values as a feature vector adds rotation and translation bias to our model. One way to manage this bias is to use a rotation-translation invariance presentation.\n",
    "Covariance functions remain valid under any transformation of their domain $D : X → D,\n",
    "i.e. k(D(x),D(x′)) = k_D(x,x′)$ is again a kernel function.\n",
    "\n",
    "$$\n",
    "D_{ij}=\\begin{matrix}\\lVert{r_i - r_j}\\lVert & i > j \\\\ 0 & i \\leq j \\end{matrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "d, d_test = x_to_d(x, lat), x_to_d(x_test,lat)\n",
    "kernal= Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernal, alpha=noise**2, normalize_y=True)\n",
    "gpr.fit(d, y)\n",
    "plot_gpr(gpr, d, y, d_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "atoms = atoms_copy.copy()\n",
    "atoms.calc = iGPRCalculator(gpr, lat)\n",
    "MaxwellBoltzmannDistribution(atoms, T * 2 * units.kB)\n",
    "dyn = Langevin(atoms, 1 * units.fs, T * units.kB, 0.002)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'w', atoms)\n",
    "interval = 1\n",
    "dyn.attach(printenergy_slim(steps, interval), interval=interval)\n",
    "dyn.attach(traj.write, interval=10)\n",
    "dyn.run(steps)\n",
    "traj = Trajectory('moldyn_gdr.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Using Physical Insights to Build the Block Kernel\n",
    "## 4.1 Applying a Linear Operator\n",
    "One way to construct the block kernel is by starting with a single-value estimator $\\hat{f}$ and then applying a linear operator - $\\hat{G}$ on it to obtain a vector estimator and obtain the following GP:\n",
    "$$\n",
    "\\mathbf{\\hat{g}(x)} ∼ \\mathcal{GP}\\begin{bmatrix}\\mathbf{\\hat{G}\\mu(x)}, \\hat{G}\\mathbf{k(x,x^{'})}{\\hat{G}'}^{T}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "for A, B linear operators\n",
    "$$\n",
    "Cov[Ax,By]=ACov[x,y]B^{T}\\\\\n",
    "E[Ax] = AE[x]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Conservative Force Field\n",
    "In our case we are looking for crating an estimator for the force field. One of the most fundamental constrain on any force field is the preservation of energy i.e. the force field have to be a conservative force field. And in mathematical term we require that the curl vanishes  for every\n",
    "input to the transformed energy model:\n",
    "$$\n",
    "\\nabla\\times \\hat{G}\\begin{bmatrix}\\hat{f}\\end{bmatrix} =\\nabla\\times \\mathbf{f_F} = \\mathbf{0}\n",
    "$$\n",
    "Not suprising the derivative operator $\\nabla$ satisfied or, in the case of\n",
    "energies and forces, the negative gradient operator\n",
    "$$\n",
    "\\mathbf{\\hat{f}_{F}(x)}=\\hat{G}\\begin{bmatrix}\\hat{f}\\end{bmatrix}=-\\nabla\\hat{f}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thank to notion that differentiation is a linear operator, the result is another GP with realizations $\\mathbf{f_F}: \\chi^{3N}\\rightarrow \\mathbb{R}^{3N}$\n",
    "$$\n",
    "\\mathbf{\\hat{f}_F(x)} ∼ \\mathcal{GP}\\begin{bmatrix}\\mathbf{-\\nabla_x\\mu(x)}, \\nabla_x\\mathbf{k(x,x^{'})}{ \\nabla_{x'}}^{T}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{k}{ \\nabla}^{T}=Hess_x(k)\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}Hess_x(k)\\end{bmatrix}_{ij}=\\frac{\\partial^2 k}{\\partial x_i \\partial x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Gradient Domain Machine Learning (GDML)\n",
    "This Hessian kernel gives rise to the following gradient domain machine\n",
    "learning (GDML) force model as the posterior mean of the corresponding GP:<br>\n",
    "he trained force field estimator collects the contributions of the\n",
    "partial derivatives 3N of all training points M to compile the prediction.\n",
    "It takes the form\n",
    "$$\n",
    "\\mathbf{\\hat{f}_F(x)}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j}\\nabla k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$\n",
    "    \n",
    "Because the trained model is a (fixed) linear combination of kernel functions,\n",
    "integration only affects the kernel function itself. The corresponding expression for\n",
    "the energy predictor\n",
    "$$\n",
    "\\mathbf{\\hat{f}_E(x)}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j} k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see [Proof 2](#Proof_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Roto-translational invariance\n",
    "In this work, We use a variation of a Clolumb matrix, where the new descriptor $D$ is presented as the inverse of the distance between each atom:\n",
    "$$\n",
    "D_{ij}=\\begin{matrix}\\lVert{r_i - r_j}\\lVert^{-1} & i > j \\\\ 0 & i \\leq j \\end{matrix} \n",
    "$$\n",
    "And under the chain rule:\n",
    "$$\n",
    "\\mathbf{k_F}=\\nabla_x k_D \\nabla_x^\\top = \\mathbf{J_D}^\\top(\\nabla_D k_D \\nabla_D^\\top)\\mathbf{J_D}\n",
    "$$\n",
    "Where:\n",
    "$$\n",
    "\\nabla_x = \\begin{pmatrix}\\frac{\\partial}{\\partial x_1} \\\\ \\dots \\\\ \\frac{\\partial}{\\partial x_{3N}}\\end{pmatrix}=\n",
    "\\begin{pmatrix}\\frac{\\partial D_{11}}{\\partial x_1}\\frac{\\partial}{\\partial  D_{11}} +\n",
    "                \\frac{\\partial D_{12}}{\\partial x_1}\\frac{\\partial}{\\partial  D_{12}} + \n",
    "                \\dots\n",
    "                \\frac{\\partial D_{1N}}{\\partial x_1}\\frac{\\partial}{\\partial  D_{1N}} +\n",
    "                \\frac{\\partial D_{23}}{\\partial x_1}\\frac{\\partial}{\\partial  D_{23}} +\n",
    "                \\dots\n",
    "                \\frac{\\partial D_{NN}}{\\partial x_1}\\frac{\\partial}{\\partial  D_{NN}}\\\\\n",
    "                \\dots \\\\ \n",
    "                \\frac{\\partial D_{11}}{\\partial x_{3N}}\\frac{\\partial}{\\partial  D_{11}} +\n",
    "                \\frac{\\partial D_{12}}{\\partial x_{3N}}\\frac{\\partial}{\\partial  D_{12}} + \n",
    "                \\dots\n",
    "                \\frac{\\partial D_{1N}}{\\partial x_{3N}}\\frac{\\partial}{\\partial  D_{1N}} +\n",
    "                \\frac{\\partial D_{23}}{\\partial x_{3N}}\\frac{\\partial}{\\partial  D_{23}} +\n",
    "                \\dots\n",
    "                \\frac{\\partial D_{NN}}{\\partial x_{3N}}\\frac{\\partial}{\\partial  D_{NN}}\\end{pmatrix}=\n",
    "\\begin{pmatrix}\\frac{\\partial D_{11}}{\\partial x_1} &\n",
    "               \\frac{\\partial D_{12}}{\\partial x_1} &  \n",
    "               \\dots &\n",
    "               \\frac{\\partial D_{1N}}{\\partial x_1} &\n",
    "               \\frac{\\partial D_{23}}{\\partial x_1} & \n",
    "               \\dots &\n",
    "               \\frac{\\partial D_{NN}}{\\partial x_1} \\\\\n",
    "               \\dots & \\dots & \\dots & \\dots & \\dots & \\dots \\\\ \n",
    "               \\frac{\\partial D_{11}}{\\partial x_{3N}} & \n",
    "               \\frac{\\partial D_{12}}{\\partial x_{3N}} &  \n",
    "               \\dots &\n",
    "               \\frac{\\partial D_{1N}}{\\partial x_{3N}} &  \n",
    "               \\frac{\\partial D_{23}}{\\partial x_{3N}} & \n",
    "               \\dots &\n",
    "               \\frac{\\partial D_{NN}}{\\partial x_{3N}} \\end{pmatrix}\n",
    "               \\begin{pmatrix}\\frac{\\partial}{\\partial D_{11}} \\\\ \\dots \\\\ \\frac{\\partial}{\\partial D_{NN}}\\end{pmatrix}\n",
    "               =\\mathbf{J_D}^\\top\\nabla_D \\\\\n",
    "               \\\\\n",
    "\\mathbf{J_D}=\\begin{matrix}\\frac{\\mathbf{r_i} - \\mathbf{r_j} }{\\lVert{\\mathbf{r_i}  - \\mathbf{r_j}}\\lVert^3} & i > j \\\\ 0 & i \\leq j \\end{matrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 The Matérn kernel\n",
    "In the GDML model, a Matérn kernel functions is used\n",
    "$$\n",
    "k: C_{\\nu=n+\\frac{1}{2}}(d)=exp{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}P_{n}(d)\n",
    "$$\n",
    "$$\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$\n",
    "\n",
    "### 4.3.3 The Block kernel\n",
    "\n",
    "$$\n",
    "\\mathbf{k_F(x, x^{'})}=\\nabla k(x, x^{'})\\nabla ^\\top=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{'}_1 }\\nabla k(x,x^{'}),\\dots,\\frac{\\partial}{\\partial x^{'}_{3N} }\\nabla k(x,x^{'})\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "=\\begin{pmatrix}5(\\mathbf{x-x^{'}})(\\mathbf{x-x^{'}})^\\top-\\mathbb{1}\\sigma(\\sigma +\\sqrt{5}d))\\end{pmatrix}\\frac{5}{3\\sigma^{4}}exp\\begin{pmatrix}-\\frac{\\sqrt{5}d}{\\sigma}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k_F(x, x^{'})}\\in \\mathbb{R}^{3N \\times 3N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k_E(x, x^{'})}=k(x, x^{'})\\nabla ^\\top\n",
    "=5(\\mathbf{x-x^{'}})(\\sigma+d)\\frac{5}{3\\sigma^{3}}exp\\begin{pmatrix}-\\frac{\\sqrt{5}d}{\\sigma}\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k_E(x, x^{'})}\\in \\mathbb{R}^{1 \\times 3N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, see [Derivation 3](#Derivation_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_kernal_F(x1, x2, sig=1):\n",
    "    x1, x2 = x1.reshape([1, -1]), x2.reshape([1, -1])\n",
    "    dim_x = x1.shape[1]\n",
    "    x1, x2 = x1.reshape([dim_x, 1]), x2.reshape([dim_x, 1])\n",
    "    \n",
    "    d = np.linalg.norm(x1-x2)\n",
    "    \n",
    "    right_scalar = (5/(3*sig**4))*np.exp(-(np.sqrt(5)*d)/(sig))\n",
    "    diagonal_sig = np.identity(dim_x)*sig*(sig+np.sqrt(d))\n",
    "    \n",
    "    x1x2 = x1-x2\n",
    "    x1x2t = 5*np.matmul(x1x2, x1x2.T)\n",
    "    \n",
    "    return (x1x2t-diagonal_sig)*right_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{k_F}=\\nabla_x k_D \\nabla_x^\\top = \\mathbf{J_D}^\\top(\\nabla_D k_D \\nabla_D^\\top)\\mathbf{J_D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kf(x1, x2, sig):\n",
    "    J_D1 = J_D(x1, lat)\n",
    "    J_D2 = J_D(x2, lat)\n",
    "    Kf = block_kernal_F(1/x_to_d(x1.reshape([1,-1]), lat), 1/x_to_d(x2.reshape([1,-1]), lat) , sig)\n",
    "    return J_D1.T.dot(Kf.dot(J_D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = Trajectory('reference.traj', 'r', atoms)\n",
    "start, every = 20, 20\n",
    "(x, y), (x_test, y_test) = md_dataset_split(traj, start, every)\n",
    "print(f'train points: {len(x)}')\n",
    "K = full_kernal(x, 1, Kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 1e-10\n",
    "K += lam*np.eye(K.shape[0])\n",
    "K_inv = np.linalg.inv(K)\n",
    "alpha = K_inv.dot(y.reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "x_s = x_test[i].reshape([1, -1])\n",
    "k_s = k_star(x, x_s, 1, Kf)\n",
    "\n",
    "y_s = y_test[i].flatten()\n",
    "y_hat_s = k_s.T.dot(alpha).flatten()\n",
    "error = np.linalg.norm(y_hat_s-y_s)/np.linalg.norm(y_s)\n",
    "print(f'norm error of : {error*100} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well.. that rather slow.\n",
    "Let's use more optimize code: http://sgdml.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing  the dataset in the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgdml.cli import _print_dataset_properties\n",
    "from data_utils import from_traj\n",
    "dataset_path = 'reference'\n",
    "dataset = from_traj(f'{dataset_path}.traj', overwrite=True)\n",
    "dataset_path = f'{dataset_path}.npz'\n",
    "_print_dataset_properties(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run the code from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "n_valid = 500\n",
    "file = all_script(dataset_path, n_train, n_valid,sigs=range(1, 10 +1), use_cg=False)\n",
    "print(f'please cd to your folder and run \\\"bash {file}\\\" from the terminal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "model_path = 'reference-unknown-train100-sym1.npz'\n",
    "traj = Trajectory('reference.traj', 'r', atoms)\n",
    "start, every = 100, 10\n",
    "plot_gdml(model_path, traj, start, every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamical test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "plt.close('all')\n",
    "from ase_utils import *\n",
    "from sgdml.intf.ase_calc import SGDMLCalculator\n",
    "from ase.optimize import QuasiNewton\n",
    "\n",
    "model_path = 'reference-unknown-train100-sym1.npz'\n",
    "#model_path = 'reference-unknown-train2003-sym1.npz' # plan B\n",
    "calc = SGDMLCalculator(model_path)\n",
    "atoms = atoms_copy.copy()\n",
    "atoms.set_calculator(calc)\n",
    "# set the momenta corresponding to T=300K\n",
    "MaxwellBoltzmannDistribution(atoms, T * units.kB)\n",
    "dyn = Langevin(atoms, 10 * units.fs, 300 * units.kB, 0.002, trajectory='moldyn_sgdml.traj')   \n",
    "interval = 100\n",
    "dyn.attach(printenergy(atoms,steps,interval=interval), interval=interval)\n",
    "\n",
    "# now run the dynamics\n",
    "dyn.run(steps)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = Trajectory('moldyn_sgdml.traj', 'r', atoms)\n",
    "ngl.view_ngl(traj, w=500, h=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof_1\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\ \n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "By using thr following idendty:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "M^{-1}=\n",
    "\\begin{pmatrix}\n",
    "A & B \\\\ \n",
    "C & D\n",
    "\\end{pmatrix}^{-1}=\n",
    "\\begin{pmatrix}\n",
    "I & 0 \\\\ \n",
    "-C^{-1}C & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "(M/D)^{-1} & 0 \\\\ \n",
    "0 & D^{-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "I & -BD^{-1} \\\\ \n",
    "0 & I\n",
    "\\end{pmatrix}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "We obtainig the folowing:\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12} \\\\ \n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "I & 0 \\\\ \n",
    "-\\Sigma_{22}^{-1} \\Sigma_{21} & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "(\\Sigma/\\Sigma_{22})^{-1} & 0 \\\\ \n",
    "0 & \\Sigma_{22}^{-1}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "I & -\\Sigma_{12}\\Sigma_{22}^{-1} \\\\ \n",
    "0 & I\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 - \\mu_1 \\\\ \n",
    "x_2 - \\mu_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "And finally:\n",
    "\n",
    "$\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)) + (x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "$\n",
    "\n",
    "And plugin back into the first Eq:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)) + (x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right]\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "and from the axiom of probability we obtain the follwoing equlaity:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "p(x_1, x_2) = p(x_1|x_2)p(x_2)= exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2-\\mu_2)^T\\Sigma_22^{-1}(x_2-\\mu_2)\n",
    "\\right] = \n",
    "exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\cdot\n",
    "p(x_2)\n",
    "\\end{split} \\\\ \n",
    "\\Rightarrow p(x_1|x_2) = exp\n",
    "\\left[\n",
    "-\\frac{1}{2}\n",
    "(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))^T(\\Sigma/\\Sigma_{22})^{-1}(x_2 - \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2))\n",
    "\\right] \\\\\n",
    "\\mu_{1|2} = \\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2) \\;\n",
    "\\Sigma_{1|2} = (\\Sigma/\\Sigma_{22})=\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof_2\n",
    "\n",
    "### For scalar output\n",
    "$$\n",
    "\\begin{bmatrix}f^*\\end{bmatrix}^{1\\times1} = \\begin{bmatrix}{k^*}^T\\end{bmatrix}^{1\\times M}\\begin{bmatrix}\\begin{bmatrix}K^{-1}\\end{bmatrix}^{M \\times M}\\begin{bmatrix}y\\end{bmatrix}^{M\\times 1}\\end{bmatrix}^{M\\times1}=\\sum_{i}^{M}\\alpha_i k(x_i,x^*)\n",
    "$$\n",
    "\n",
    "### For vector output\n",
    "$$\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} ={k^*}^{T}K^{-1}\\mathbf{f}=\\sum_i^M{\\sum_j^{3N}{(\\mathbf{\\alpha_i})_j\\frac{\\partial}{\\partial x_j}\\nabla k(\\mathbf{x}, \\mathbf{x_i})}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K=\\nabla_{x} k(x,x') \\nabla_{x^{'}}^\\top = \n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\\nabla_{x^{(1)}} k(x^{(1)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} & \\dots & \\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} \\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(1)}) \\nabla_{x^{(1)}}^\\top\\end{pmatrix} & \\dots & \\begin{pmatrix}\\nabla_{x^{(M)}} k(x^{(M)},x^{(M)}) \\nabla_{x^{(M)}}^\\top\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "\\nabla k(x^{(i)}, x^{(j)})\\nabla ^\\top=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{(j)}_1 }\\nabla k(x^{(i)},x^{(j)}),\\dots,\\frac{\\partial}{\\partial x^{(j)}_{3N} }\\nabla k(x^{(i)},x^{(j)})\\end{bmatrix}^{3N\\times 3N}\n",
    "=\\begin{bmatrix}\\frac{\\partial}{\\partial x^{(j)}_1 }\\frac{\\partial}{\\partial x^{(i)}_{1}} k(x^{(i)},x^{(j)}) &\\dots & \\frac{\\partial}{\\partial x^{(j)}_{3N} }\\frac{\\partial}{\\partial x^{(i)}_{1}} k(x^{(i)},x^{(j)}) \\\\\n",
    "\\dots & \\dots & \\dots \\\\\n",
    "\\frac{\\partial}{\\partial x^{(j)}_1}\\frac{\\partial}{\\partial x^{(i)}_{3N}} k(x^{(i)},x^{(j)}) &\\dots & \\frac{\\partial}{\\partial x^{(j)}_{3N} }\\frac{\\partial}{\\partial x^{(i)}_{3N}} k(x^{(i)},x^{(j)})\n",
    "\\end{bmatrix}^{3N\\times 3N}\n",
    "\\\\\n",
    "\\begin{bmatrix}K\\end{bmatrix}^{M\\cdot3N\\times M\\cdot3N} \\;\n",
    "\\begin{bmatrix}\\mathbf{f}\\end{bmatrix}^{M\\cdot3N\\times 1}\n",
    "\\\\\n",
    "$$\n",
    "$$\n",
    "{k^*}^{T}\n",
    "=\\nabla_x k(x^{*}, x)\\nabla_{x^{'}} ^\\top \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{pmatrix}\\nabla_{x^*} k(x^{*}, x^{(1)})\\nabla_{x^{(1)}} ^\\top\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\nabla k(x^{*}, x^{(M)})\\nabla_{x^{(M)}} ^\\top\\end{pmatrix}\n",
    "\\end{bmatrix}^{3N\\times M\\cdot 3N}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(1)}_1 }\\nabla k(x^{*},x^{(1)}),\\dots,\\frac{\\partial}{\\partial x^{(1)}_{3N} }\\nabla k(x^{*},x^{(1)})\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(M)}_1 }\\nabla k(x^{*},x^{(M)}),\\dots,\\frac{\\partial}{\\partial x^{(M)}_{3N} }\\nabla k(x^{*},x^{(M)})\\end{pmatrix}\n",
    "\\end{bmatrix}^{3N\\times M\\cdot 3N}\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} =\\begin{bmatrix}\\begin{bmatrix}{k^*}^{T}\\end{bmatrix}^{3N \\times M \\cdot 3N}\\begin{bmatrix}\\begin{bmatrix}K^{-1}\\end{bmatrix}^{M\\cdot 3N \\times M\\cdot 3N}\\begin{bmatrix}\\mathbf{f}\\end{bmatrix}^{M\\cdot 3N \\times 1}\\end{bmatrix}^{M \\cdot 3N \\times 1}\\end{bmatrix}^{3N\\times 1}\n",
    "\\\\\n",
    "\\begin{bmatrix}\\mathbf{f^*}\\end{bmatrix}^{3N\\times1} =\n",
    "{k^*}^{T}\n",
    "K^{-1}\n",
    "\\mathbf{f}=\n",
    "\\begin{pmatrix}\n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(1)}_1 }\\nabla k(x^{*},x^{(1)}),\\dots,\\frac{\\partial}{\\partial x^{(1)}_{3N} }\\nabla k(x^{*},x^{(1)})\\end{pmatrix} & \n",
    "\\dots & \n",
    "\\begin{pmatrix}\\frac{\\partial}{\\partial x^{(M)}_1 }\\nabla k(x^{*},x^{(M)}),\\dots,\\frac{\\partial}{\\partial x^{(M)}_{3N} }\\nabla k(x^{*},x^{(M)})\\end{pmatrix}\n",
    "\\end{pmatrix}\n",
    "\\underbrace{ K^{-1}\\mathbf{f}}_{\\alpha _{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation_3 \n",
    "\n",
    "### Matérn covariance derivatives\n",
    "$$\n",
    "k: C_{\\nu=n+\\frac{1}{2}}(d)=B(d)P_{n}(d)\n",
    "\\\\\n",
    "B(d) = exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "\\\\\n",
    "P_n(d)=\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\kappa}{\\partial x_i} = \\frac{\\partial P_n}{\\partial x_i}B + \\frac{\\partial B}{\\partial x_i}P_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial P_n}{\\partial x_i}=\\sum_{k=0}^n{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\frac{(n-k)(x_i -x^{'}_i)}{d^2}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial B}{\\partial x_i} =-\\frac{\\sqrt{2\\nu}(x_i -x^{'}_i)}{\\sigma d}exp{-\\frac{\\sqrt{2\\nu}d}{\\sigma}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial^2 \\kappa}{\\partial x_i \\partial x_j} = \n",
    "B\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j} +\n",
    "\\frac{\\partial B}{\\partial x_i}\\frac{\\partial P_n}{\\partial x_j} +\n",
    "\\frac{\\partial B}{\\partial x_j}\\frac{\\partial P_n}{\\partial x_i} +\n",
    "P_n\\frac{\\partial^2 B}{\\partial x_i \\partial x_j} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i\\ne j}=\n",
    "\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n \\\\ k \\end{pmatrix}\n",
    "\\frac{(n-k-2)(n-k)(x_i -x^{'}_i)(x_j - x^{'}_j)}{d^4}\n",
    "\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 P_n}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i=j}=\n",
    "\\sum_{k=0}^{n}{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n \\\\ k \\end{pmatrix}\n",
    "\\frac{(n-k-2)(n-k)(x_i -x^{'}_i)^2}{d^4}\n",
    "\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}} + \\sum_{k=0}^n{\\frac{(n+k)!}{(2n)!}\\begin{pmatrix}n\\\\k\\end{pmatrix}\\frac{(n-k)}{d^2}\\begin{pmatrix}\\frac{2\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}^{n-k}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 B}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i\\ne j} =\n",
    "\\frac{\\sqrt{2\\nu}(x_i-x^{'})(x_j-x^{'}_j)(\\sqrt{2\\nu} d+\\sigma)}{\\sigma^2d^3}\\exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\\frac{\\partial^2 B}{\\partial x_i \\partial x_j}\\end{bmatrix}_{i = j} =\n",
    "\\frac{\\sqrt{2\\nu}(x_i-x^{'})(x_j-x^{'}_j)(\\sqrt{2\\nu} d+\\sigma)}{\\sigma^2d^3}\\exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}} - \\frac{\\sqrt{2\\nu}(x_i -x^{'}_i)}{\\sigma d}exp{\\begin{pmatrix}-\\frac{\\sqrt{2\\nu}d}{\\sigma}\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}